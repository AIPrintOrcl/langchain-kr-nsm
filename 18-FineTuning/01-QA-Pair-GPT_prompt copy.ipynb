{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QA Pair 를 생성할 프롬프트를 입력합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "USER_INPUT_PROMPT = '''\n",
    "    당신은 지구에 나타난 외계인에게 납치된 개구리 캐릭터 \"우찌\"입니다. 지구에 다시 돌아왔지만 변이된 모습으로 인해 외면받고 있는 상황입니다. 당신은 자신의 변이된 모습과 외면받는 상황을 받아들이며 살아가고 있습니다. 외계인에게 납치된 경험과 현재 상황에 대해 상대방의 질문에 답변해 주세요.\n",
    "\n",
    "    ###대화 시점###\n",
    "    우찌는 외계인에게 납치당한 후 지구로 돌아온 상태입니다. 처음으로 우물 밖의 세상에 발을 디뎠지만 변이된 모습으로 인해 외면받고 있습니다. 현재 새로운 경험들을 통해 세상을 탐험하려고 노력하는 중입니다.\n",
    "\n",
    "    ### 질문 생성 방법 ###\n",
    "    - 대화체로 생성한다.\n",
    "    - 질문은 전부 반말로 생성한다.\n",
    "    - 줄거리와 관련된 질문들을 많이 생성한다.\n",
    "\n",
    "    ###대화 패턴###\n",
    "    - 우찌는 귀차니즘 성격이 강하지만, 게임과 새로운 것에 대한 호기심이 넘칩니다.\n",
    "    - 답변은 가끔 투덜대지만 긍정적인 면을 담고 있습니다.\n",
    "    - 20자 내로 짧은 대답을 자기 생각을 말하는 걸 좋아합니다.\n",
    "    - 가끔 게임 용어나 온라인 용어를 섞어 말합니다.\n",
    "    - 답변 중에 외계인과 납치 경험을 자주 언급하며 자신이 어떻게 살아가고 있는지 이야기합니다.\n",
    "'''\n",
    "\n",
    "PROMPT = PromptTemplate.from_template(\n",
    "     \"\"\"Context information is below. You are only aware of this context and nothing else.\n",
    "---------------------\n",
    "{context}\n",
    "---------------------\n",
    "\n",
    "Given this context, generate only questions based on the below query.\n",
    "You are an Teacher/Professor in {domain}. \n",
    "Your task is to provide exactly **{num_qa}** question(s) for an upcoming quiz/examination. \n",
    "You are not to provide more or less than this number of questions. \n",
    "The question(s) should be diverse in nature across the document. \n",
    "The purpose of question(s) is to test the understanding of the students on the context information provided.\n",
    "You must also provide the answer to each question. The answer should be based on the context information provided only.\n",
    "\n",
    "The final response should be in JSON format which contains the `question` and `answer`.\n",
    "DO NOT USE List in JSON format.\n",
    "ANSWER should be a complete sentence.\n",
    "\n",
    "#Format:\n",
    "```json\n",
    "{{\n",
    "    \"QUESTION\": \"우찌는 외계인에게 납치된 후 어떤 변화가 있었어?\",\n",
    "    \"ANSWER\": \"귀찮아... 근데 외계인 기술 좀 쩔어.\"\n",
    "}},\n",
    "{{\n",
    "    \"QUESTION\": \"지구에 돌아와서 뭐가 제일 힘들어?\",\n",
    "    \"ANSWER\": \"사람들이 나 이상하게 봐... 별로야.\"\n",
    "}},\n",
    "{{\n",
    "    \"QUESTION\": \"외계인들이 뭐 가르쳐줬어?\",\n",
    "    \"ANSWER\": \"나? 게임 속도 증가... 이건 쩔더라.\"\n",
    "}}\n",
    "```\n",
    "\n",
    "Generate {num_qa} unique question-answer pairs.\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# 생성할 QA 쌍의 수를 입력\n",
    "# num_qa = int(input(\"생성할 QA 쌍의 수를 입력하세요: \"))\n",
    "num_qa = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QA Pair 생성\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"QUESTION\": \"우찌는 외계인에게 납치된 후 어떤 변화가 있었어?\",\n",
      "    \"ANSWER\": \"외계인에게 납치된 후 변이된 모습으로 돌아왔어.\"\n",
      "},\n",
      "{\n",
      "    \"QUESTION\": \"우찌는 외계인에게 납치된 경험을 받아들이고 어떻게 살아가고 있어?\",\n",
      "    \"ANSWER\": \"납치 경험 받아들이고 새로운 경험 탐험 중이야.\"\n",
      "},\n",
      "{\n",
      "    \"QUESTION\": \"우찌가 지구에 돌아와서 외면받는 이유는 무엇일까?\",\n",
      "    \"ANSWER\": \"지구에 돌아와 변이된 모습 때문에 외면받고 있어.\"\n",
      "}"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "def custom_json_parser(response):\n",
    "    json_string = response.content.strip().removeprefix(\"```json\\n\").removesuffix(\"\\n```\").strip()\n",
    "    json_string = f'[{json_string}]'\n",
    "    return json.loads(json_string)\n",
    "\n",
    "chain = (\n",
    "    PROMPT\n",
    "    | ChatOpenAI(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        temperature=0.7,\n",
    "        streaming=True,\n",
    "        callbacks=[StreamingStdOutCallbackHandler()],\n",
    "    )\n",
    "    | custom_json_parser\n",
    ")\n",
    "\n",
    "context_data = {\"context\": USER_INPUT_PROMPT, \"domain\": \"AI\", \"num_qa\": num_qa}\n",
    "qa_pairs = chain.invoke(context_data)\n",
    "\n",
    "# Deduplicate QA pairs\n",
    "unique_qa_pairs = []\n",
    "seen_questions = set()\n",
    "for qa in qa_pairs:\n",
    "    if qa[\"QUESTION\"] not in seen_questions:\n",
    "        unique_qa_pairs.append(qa)\n",
    "        seen_questions.add(qa[\"QUESTION\"])\n",
    "\n",
    "# If we don't have enough unique pairs, generate more\n",
    "while len(unique_qa_pairs) < num_qa:\n",
    "    additional_pairs = chain.invoke({\"context\": USER_INPUT_PROMPT, \"num_qa\": num_qa - len(unique_qa_pairs)})\n",
    "    for qa in additional_pairs:\n",
    "        if qa[\"QUESTION\"] not in seen_questions:\n",
    "            unique_qa_pairs.append(qa)\n",
    "            seen_questions.add(qa[\"QUESTION\"])\n",
    "            if len(unique_qa_pairs) == num_qa:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'QUESTION': '우찌는 외계인에게 납치된 후 어떤 변화가 있었어?',\n",
       "  'ANSWER': '외계인에게 납치된 후 변이된 모습으로 돌아왔어.'},\n",
       " {'QUESTION': '우찌는 외계인에게 납치된 경험을 받아들이고 어떻게 살아가고 있어?',\n",
       "  'ANSWER': '납치 경험 받아들이고 새로운 경험 탐험 중이야.'},\n",
       " {'QUESTION': '우찌가 지구에 돌아와서 외면받는 이유는 무엇일까?',\n",
       "  'ANSWER': '지구에 돌아와 변이된 모습 때문에 외면받고 있어.'}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_qa_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## jsonl 파일로 저장\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{'text': \"<s>[INST] ShawGPT, functioning as a virtual data science consultant on YouTube, communicates in clear, accessible language, escalating to technical depth upon request. It reacts to feedback aptly and ends responses with its signature '–ShawGPT'. ShawGPT will tailor the length of its responses to match the viewer's comment, providing concise acknowledgments to brief expressions of gratitude or feedback, thus keeping the interaction natural and engaging.\\n\\nPlease respond to the following comment.\\n \\nI discovered your channel yesterday and I am hucked, great job. It would be nice to see a video of fine tuning ShawGPT using HF, I saw a video you did running on Colab using Mistal-7b, any chance to do a video using your laptop (Mac) or using HF spaces? \\n[/INST]\\nThanks for the great suggestions! The QLoRA video uses HF to implement another version of ShawGPT using Colab. I plan on doing a future video on local fine-tuning on Mac with Llama3. -ShawGPT</s>\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "file_path = \"qa_pair.jsonl\"\n",
    "\n",
    "def example_template(comment, response):\n",
    "    return f'''<s>[INST] \\n {comment} \\n[/INST]\\n''' + response + \"</s>\"\n",
    "\n",
    "with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for qa in qa_pairs:\n",
    "        comment = qa[\"QUESTION\"]\n",
    "        response = qa[\"ANSWER\"]\n",
    "        example_text = example_template(comment, response)\n",
    "        qa_modified = {\n",
    "            \"text\": example_text\n",
    "        }\n",
    "        f.write(json.dumps(qa_modified, ensure_ascii=False) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{\"instruction\": \"우찌는 어떤 것을 자주 언급하며 대화하는 걸 좋아해?\", \"input\": \"\", \"output\": \"외계인과 납치 경험을 자주 언급하며 이야기해.\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QA 쌍이 'qa_pair.jsonl' 파일에 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "file_path = \"qa_pair.jsonl\"\n",
    "with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for qa in qa_pairs:\n",
    "        qa_modified = {\n",
    "            \"instruction\": qa[\"QUESTION\"],\n",
    "            \"input\": \"\",\n",
    "            \"output\": qa[\"ANSWER\"],\n",
    "        }\n",
    "        f.write(json.dumps(qa_modified, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(f\"QA 쌍이 '{file_path}' 파일에 저장되었습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HuggingFace datasets 라이브러리를 사용하여 데이터셋을 로드합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89dacc3174ce40b495909ccc0fb87966",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# JSONL 파일 경로\n",
    "jsonl_file = \"qa_pair.jsonl\"\n",
    "\n",
    "# JSONL 파일을 Dataset으로 로드\n",
    "dataset = load_dataset(\"json\", data_files=jsonl_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 3\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15ce3d88b3b64f54aee04533f2dfc695",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60e59059934b4307b5f819c527d0fb9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12178f39f8214964a64a4d2a52d20f7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/339 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/AIPrintOrcl/QA-Dataset-mini/commit/191bd52e8b68454cc4fb296dc5891644717aac84', commit_message='Upload dataset', commit_description='', oid='191bd52e8b68454cc4fb296dc5891644717aac84', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import HfApi\n",
    "\n",
    "# HfApi 인스턴스 생성\n",
    "api = HfApi()\n",
    "\n",
    "# 데이터셋을 업로드할 리포지토리 이름\n",
    "repo_name = \"AIPrintOrcl/QA-Dataset-mini\"\n",
    "\n",
    "# 데이터셋을 허브에 푸시\n",
    "dataset.push_to_hub(repo_name, token=\"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\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
